---
title: "<center> <h1>Homework 2</h1> </center>"
author: "Raul Torres Aragon"
date: "10/12/2022"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
set.seed(570)
```

## 1.  

Here we investigate the robustness of the OLS estimator to non-normality of the errors.
Consider the simple linear regression model
$$
Y_i = \beta_0 + \beta_1 x_i + \epsilon_i, i=1,...,n
$$

where the error terms $\epsilon_i$ are such that $E[\epsilon_i] = 0$, $var(\epsilon_i) = \sigma^2$ and $cov(\epsilon_i,\epsilon_j) = 0$, $i \neq j$. Next, consider a covariate $x_i$ with: $x_i \sim_{\text{iid}} \mathcal{N}(20,22)$, with $\beta_0 = 3$ and $\beta_11 =âˆ’3$ and $n=1,540$.  

We want to simulate model (1) 1,000 times but with the errors $\epsilon_i$ i.i.d. from the following distributions:  

* The normal distribution with mean 0 and variance $2^2$.  
* The uniform distribution on the range $(-r, r)$ for $r = 5$.  
* A skew normal distribution with $\alpha = 5$, $\omega = 1$, and $\xi = \frac{-5}{\sqrt{13pi}}$, which will give zero mean.  

We explain the simulation procedure.  
First we set seed to 570 for replicability. We then generate 15 draws from the normal distribution with mean = 20 and standard deviation = 2 to create $X\sim N(20, 2^2)$ our fixed $X$ variable. This variable will not be generated multiple times. (We use `rnorm(20, 2, n = 15)` from R. We then create the true parameters $\beta_0 = 3$ and $\beta_1 = -3$.  
We then iterate 5,000 times. On each iteration, we create a set of three $Y$ vectors of length 15. Each vector is composed of the same linear coefficients $\beta_0$ and $\beta_1$ and $X$ variable, but they differ in the distribution of the disturbance term as follows:  
1) `y = b0 + b1 * x + rnorm(mean=0, sd=2, n=15)`  
2) `y = b0 + b1 * x + runif(n=15, min=-5, max=5)`  
3) `y = b0 + b1 * x + sn::rsn(alpha=5, omega=1, xi=-5/(sqrt(13*pi)), n=15)[1:15]`  

After creating the three vectors, still in the same iteration, we regress each constructed $Y$ vector on $x$ using R's `lm()` function. After running each regression (there are three regression per iteration), we collect the following statistics and storing it in an R list:   

* Bias $E(\hat{y} - y)$ where $\hat{y}$ is the predicted values from the regression.    
* $\beta_0$ estimate  
* $\beta_1$ estimate  
* Variance of the $\beta_0$ estimate  
* Variance of the $\beta_1$ estimate  
* 80% confidence interval of $\beta_0$  
* 80% confidence interval of $\beta_1$  

This marks the end of the iteration. We then iterate 5,000 more times.  
After doing the above, we repeat the whole process but setting the length of the $X$ and $Y$ vectors to 40.  


```{r include = FALSE}
Nsim <- 1000

run_sims <- function(Nsim = Nsim, b0=3, b1=-3, n=15) {
  
 x <- rnorm(mean=20, sd=2, n=n)

 ModName      <- vector(mode = "character", length = n*3)
 Biases       <- vector(mode = "numeric", length = n*3)
 Var_beta0    <- vector(mode = "numeric", length = n*3)
 Var_beta1    <- vector(mode = "numeric", length = n*3)
 Beta0s       <- vector(mode = "numeric", length = n*3)
 Beta1s       <- vector(mode = "numeric", length = n*3)
 CI_beta0_low <- vector(mode = "numeric", length = n*3) 
 CI_beta0_hig <- vector(mode = "numeric", length = n*3)
 CI_beta1_low <- vector(mode = "numeric", length = n*3)
 CI_beta1_hig <- vector(mode = "numeric", length = n*3)
 
 ynames <- c("Y_norm","Y_unif","Y_skew")
 j = 0
 for(i in 1:Nsim) {
   
   y_norm <- b0 + b1*x + rnorm(mean=0, sd=2, n=n)
   y_unif <- b0 + b1*x + runif(min=-5, max=5, n=n)
   y_skew <- b0 + b1*x + sn::rsn(alpha=5, omega=1, xi=-5/(sqrt(13*pi)), n=n)[1:n]
   
   ys <- list(y_norm, y_unif, y_skew)
   
   k = 0
   for(y in ys){
     k = k+1
     j = j+1
     mymod <- lm(y~x)
     yhat  <- predict(mymod)
     bias  <- mean(yhat - y)
     CIs   <- confint(mymod, param = c(0,1), level = 0.80)
     
     ModName[j]   <- paste(ynames[k], i)
     Biases[j]    <- bias
     Var_beta0[j] <- vcov(mymod)[1,1]
     Var_beta1[j] <- vcov(mymod)[2,2]
     Beta0s[j]    <- mymod$coefficients[[1]]
     Beta1s[j]    <- mymod$coefficients[[2]]
     CI_beta0_low[j] <- CIs[1]
     CI_beta1_low[j] <- CIs[2]
     CI_beta0_hig[j] <- CIs[3]
     CI_beta1_hig[j] <- CIs[4]    
     
   }
 }
 
 results <- dplyr::tibble(model = ModName, 
                          bias = Biases,
                          var_beta0 = Var_beta0,
                          var_beta1 = Var_beta1,
                          b0 = Beta0s,
                          b1 = Beta1s,
                          b0_ci_low = CI_beta0_low,
                          b0_ci_hig = CI_beta0_hig,
                          b1_ci_low = CI_beta1_low,
                          b1_ci_hig = CI_beta1_hig                         
 )
 results$type <- stringr::str_sub(results$model, 3, 6)
 results$b0_inside <- 0
 results$b0_inside[results$b0_ci_low < b0 & b0 < results$b0_ci_hig] <- 1
 results$b1_inside <- 0
 results$b1_inside[results$b1_ci_low < b1 & b1 < results$b1_ci_hig] <- 1
 
 
 # confirm numerically that the bias is zero
 bias_results <- sum(results$bias != 0)

 # compare variance of betas with the sampling distribution of betas_hat
 sampl_var_b0_norm <- results$b0[results$type == "norm"] |> var() |> round(3)
 sampl_var_b1_norm <- results$b1[results$type == "norm"] |> var() |> round(3)
 
 sampl_var_b0_unif <- results$b0[results$type == "unif"] |> var() |> round(3)
 sampl_var_b1_unif <- results$b1[results$type == "unif"] |> var() |> round(3)
 
 sampl_var_b0_skew <- results$b0[results$type == "skew"] |> var() |> round(3)
 sampl_var_b1_skew <- results$b1[results$type == "skew"] |> var() |> round(3)
 
 mean_varb0_norm <- results$var_beta0[results$type == "norm"] |> mean() |> round(3)
 mean_varb0_unif <- results$var_beta0[results$type == "unif"] |> mean() |> round(3)
 mean_varb0_skew <- results$var_beta0[results$type == "skew"] |> mean() |> round(3)
 
 mean_varb1_norm <- results$var_beta1[results$type == "norm"] |> mean() |> round(3)
 mean_varb1_unif <- results$var_beta1[results$type == "unif"] |> mean() |> round(3)
 mean_varb1_skew <- results$var_beta1[results$type == "skew"] |> mean() |> round(3)
 
 # examine the distribution of the resultant estimators (across sims) of betas...
 ci_b0_norm_inside <- mean(results[results$type == "norm", ]$b0_inside)
 ci_b0_unif_inside <- mean(results[results$type == "unif", ]$b0_inside)
 ci_b0_skew_inside <- mean(results[results$type == "skew", ]$b0_inside)
 ci_b1_norm_inside <- mean(results[results$type == "norm", ]$b1_inside)
 ci_b1_unif_inside <- mean(results[results$type == "unif", ]$b1_inside)
 ci_b1_skew_inside <- mean(results[results$type == "skew", ]$b1_inside)
 
 
 o <- list("bias_results"      = bias_results, 
           "ci_b0_norm_inside" = ci_b0_norm_inside,
           "ci_b0_unif_inside" = ci_b0_unif_inside,
           "ci_b0_skew_inside" = ci_b0_skew_inside,
           "ci_b1_norm_inside" = ci_b1_norm_inside,
           "ci_b1_unif_inside" = ci_b1_unif_inside,
           "ci_b1_skew_inside" = ci_b1_skew_inside,
           "sampl_var_b0_norm" = sampl_var_b0_norm,
           "sampl_var_b1_norm" = sampl_var_b1_norm,
           "sampl_var_b0_unif" = sampl_var_b0_unif,
           "sampl_var_b1_unif" = sampl_var_b1_unif,
           "sampl_var_b0_skew" = sampl_var_b0_skew,
           "sampl_var_b1_skew" = sampl_var_b1_skew,
           "mean_varb0_norm"   = mean_varb0_norm,
           "mean_varb0_unif"   = mean_varb0_unif,
           "mean_varb0_skew"   = mean_varb0_skew,
           "mean_varb1_norm"   = mean_varb1_norm,
           "mean_varb1_unif"   = mean_varb1_unif,
           "mean_varb1_skew"   = mean_varb1_skew,           
           "results_df"        = results
           )
 
o
 
} # end function

o15 <- run_sims(Nsim = Nsim, n=15)
o40 <- run_sims(Nsim = Nsim, n=40)

```


### (a)  
We confirm numerically that the bias is zero on each iteration as the following table shows.  

```{r, echo = FALSE}
knitr::kable(
dplyr::tibble("parameter" = c("b0","b1"), 
       "bias (norm)" = c("<0.001","<0.001"),
       "bias (unif)" = c("<0.001","<0.001"),
       "bias (skew)" = c("<0.001","<0.001")))

```
$E(\hat{y}-y) = 0$ for all models, whether n=15 or n=40.  

### (b)  
When comparing the variance of each $\beta$ estimator as reported by least squares, with that which follows from the sampling distribution of the estimator (which we obtained by computing the variance of all the $\beta_0$ and $\beta_1$ coefficients through the entire simulation but within its disturbance-distribution class).  

The below figures show histograms of each $\beta_0$ and $\beta_1$ coefficient within its disturbance class. The variance reported by `lm()` for all regressions and the sample variance within disturbance class for the simulation with vector length of 15 are reported below each histogram. (The results for vector length 40 are similar).  


```{r, echo = FALSE}
par(mfrow = c(1,3))

hist(o15$results_df$b0[o15$results_df$type == "norm"], main = "b0 normal epsilon", 
     xlab = paste0("mean(lm var) = ", 
                   round(o15$mean_varb0_norm,2), "\n sample var =", 
                   round(o15$sampl_var_b0_norm,2)))

hist(o15$results_df$b0[o15$results_df$type == "unif"], main = "b0 normal epsilon", 
     xlab = paste0("mean(lm var) = ", 
                   round(o15$mean_varb0_unif,2), "\n sample var =", 
                   round(o15$sampl_var_b0_unif,2)))

hist(o15$results_df$b0[o15$results_df$type == "skew"], main = "b0 normal epsilon", 
     xlab = paste0("mean(lm var) = ", 
                   round(o15$mean_varb0_skew,2), "\n sample var =", 
                   round(o15$sampl_var_b0_skew,2)))


```  
```{r, echo = FALSE}
par(mfrow = c(1,3))

hist(o15$results_df$b1[o15$results_df$type == "norm"], main = "b1 normal epsilon", 
     xlab = paste0("mean(lm var) = ", 
                   round(o15$mean_varb1_norm,2), "\n sample var =", 
                   round(o15$sampl_var_b1_norm,2)))

hist(o15$results_df$b1[o15$results_df$type == "unif"], main = "b1 normal epsilon", 
     xlab = paste0("mean(lm var) = ", 
                   round(o15$mean_varb1_unif,2), "\n sample var =", 
                   round(o15$sampl_var_b1_unif,2)))

hist(o15$results_df$b1[o15$results_df$type == "skew"], main = "b1 normal epsilon", 
     xlab = paste0("mean(lm var) = ", 
                   round(o15$mean_varb1_skew,2), "\n sample var =", 
                   round(o15$sampl_var_b1_skew,2)))
```

The R reported $\boldsymbol{\hat{\beta}}$ variances are close to the sample parameter variances, whether n=15 or n=40. In other words, the mean of the OLS-reported variances is practically identical to the variance of all my simulated $\boldsymbol{\hat{\beta}}$ (sample variance for $\boldsymbol{\hat{\beta}}$). Again, this is true whether n=15 or n=40.   

### (c)  
Examining the distribution of the resultant estimators across simulations (See histograms above) of $\beta_0$ and $\beta_1$, we see that the distributions are normal-looking, despite of the underlying distribution of the disturbance term.  

The below qq-plots lend more credibility to the normality of the distribution of estimates.  

```{r, echo = FALSE}
par(mfrow = c(1,2))

qqnorm(o40$results_df$b0[o40$results_df$type == "norm"], pch = 1, 
       frame = FALSE, main = "b0 normal")
qqline(o40$results_df$b0[o40$results_df$type == "norm"], col = "steelblue", lwd = 2)

qqnorm(o40$results_df$b1[o40$results_df$type == "norm"], pch = 1, 
       frame = FALSE, main = "b1 normal")
qqline(o40$results_df$b1[o40$results_df$type == "norm"], col = "steelblue", lwd = 2)
```

```{r, echo = FALSE}
par(mfrow = c(1,2))
qqnorm(o40$results_df$b0[o40$results_df$type == "unif"], 
       pch = 1, frame = FALSE, main = "b0 unif")
qqline(o40$results_df$b0[o40$results_df$type == "unif"], col = "steelblue", lwd = 2)

qqnorm(o40$results_df$b1[o40$results_df$type == "unif"], pch = 1, 
       frame = FALSE, main = "b1 unif")
qqline(o40$results_df$b1[o40$results_df$type == "unif"], col = "steelblue", lwd = 2)
```

```{r, echo = FALSE}
par(mfrow = c(1,2))
qqnorm(o40$results_df$b0[o40$results_df$type == "skew"], pch = 1, 
       frame = FALSE, main = "b0 skew")
qqline(o40$results_df$b0[o40$results_df$type == "skew"], col = "steelblue", lwd = 2)

qqnorm(o40$results_df$b1[o40$results_df$type == "skew"], pch = 1, 
       frame = FALSE, main = "b1 skew")
qqline(o40$results_df$b1[o40$results_df$type == "skew"], col = "steelblue", lwd = 2)

```

The figures below show, for each parameter find the coverage probability of an 80% confidence interval, that is the proportion of times that the confidence intervals contain the true value.  

```{r, echo = FALSE}
sn <- 25

samp <- sample(1:1000, size = sn)

par(mfrow = c(1,2))

plotrix::plotCI(x=1:sn, rep(-3,sn), 
                abs(o15$results_df$b0_ci_low[samp]), 
                abs(o15$results_df$b0_ci_hig[samp]), 
                main = "25 randomly selected \n80% CI for b0", ylab = "", xlab = "")

plotrix::plotCI(x=1:sn, rep(-3,sn), 
                abs(o15$results_df$b1_ci_low[samp]), 
                abs(o15$results_df$b1_ci_hig[samp]), 
                main = "25 randomly selected \n80% CI for b1", ylab = "", xlab = "")


```  

Regardless of whether Y was built with normally, uniformly, or skew-normally distributed disturbances with mean 0, the true parameter vector $\boldsymbol{\beta}$ was captured within the 80% confidence intervals, 80% of the time.  


## 2.  
Consider the exponential regression problem with independent responses  

$$ 
p(y|\lambda_i) = \lambda_ie^{-\lambda_iy}, y>0
$$   
and $log\lambda_i = \beta_0 + \beta_1x_i$ for given covariates $x_i$, $i=1,...,n$. We wish to estimate the $2\times1$ regression parameter $\boldsymbol{\beta} = [\beta_0, \beta_1]^T$ using MLE.  

### (a)  
Firs we find expressions for the likelihood function $\mathcal{L}({\beta})$, log likelihood function $\mathcal{l}(\beta)$, score function $\mathcal{S}(\beta)$ and Fisher's information matrix $\mathcal{I}(\beta)$. 

The likelihood function $L(\boldsymbol{\beta})$ is the product of the density function over the number of observations. (Recall that $log(\lambda_i) = \beta_0 + \beta_1x_i$ and thus, $\lambda_i = exp[\beta_0 + \beta_1x_i]$.  
$$
L(\boldsymbol{\beta}) = \\
\prod_{i=1}^n p(y|\lambda_i) = \\
\prod_{i=1}^n \lambda_ie^{-\lambda_iy} = \\
\prod_{i=1}^n exp[\beta_0 + \beta_i x_i + y_iexp{(-\beta_0 - \beta_1x_i)}] = \\
exp\biggl[\sum_{i=1}^n (\beta_0 + \beta_1 x_i)-\sum_{i=0}^ny_iexp(\beta_0 + \beta_1 x_i)\biggr]

$$  

The log-likelihood function $l(\boldsymbol{\beta}) = log(L(\boldsymbol{\beta}))=$  

$$
\sum_i^n [\beta_0 + \beta_i x_i]-\sum_{i=0}^ny_ie^{(\beta_0+\beta_1 x_i)}
$$  

The vector-valued score function $\boldsymbol{S}(\boldsymbol{\beta}) = [\frac{\partial{l(\boldsymbol{\beta})}}{\partial\beta_0}, \frac{\partial{l(\boldsymbol{\beta})}}{\partial\beta_1}] = $  

$$
\begin{bmatrix} 
n-\sum_{i=1}^n e^{\beta_0+\beta_1 x_i y} & \sum_{i=1}^nx_i-\sum_{i=1}^n y_ix_ie^{\beta_0+\beta_1x_i}
\end{bmatrix} 
$$  

The Fisher Information Matrix is defined as $\mathcal{I}(\boldsymbol{\beta}) = -E[\frac{\partial{l(\boldsymbol{\beta})}}{\partial{\beta_i}\partial{\beta_j}}]$  

In our example, and noticing that the expected value $e^{\beta_0 + \beta_1 x_i}$ is $\lambda_i$ which is the expectation of $y_i$:  

$$ 
\begin{bmatrix}
-E[\frac{\partial{l(\boldsymbol{\beta})}}{\partial{\beta_0}\partial{\beta_0}}] & -E[\frac{\partial{l(\boldsymbol{\beta})}}{\partial{\beta_0}\partial{\beta_1}}] \\
-E[\frac{\partial{l(\boldsymbol{\beta})}}{\partial{\beta_1}\partial{\beta_0}}]  & -E[\frac{\partial{l(\boldsymbol{\beta})}}{\partial{\beta_1}\partial{\beta_1}}]
\end{bmatrix} = \begin{bmatrix}
n & \sum_{i=1}^n x_i^2 \\
\sum_{i=1}^n x_i^2  & \sum_{i=1}^n x_i
\end{bmatrix}
$$

### (b)  
WE now find expressions for the maximum likelihood estimate $\hat{\beta}$. However, given that there is no closed-form solution, we provide a functional form that could be simply implemented for solution. 
We have to solve for $\boldsymbol{\beta}$ vector and set it equal to zero. There is no closed-form solution for $\hat{\beta}$. But the `optim` function of R could find could find a solution:  

```
loglike <- function(betas){
  sum(betas[1] + betas[2] * x - exp((betas[1] + betas[2] * x)*y))
}

optim(par = c(1,1),
      fn = loglike,
      control = list(fnscale = -1)) 
```


### (c)  
For the data in Table 1, numerically maximize the likelihood function to obtain estimates of $\beta$. These data consist of the survival times (y) of rats as a function of concentrations of a contaminant (x). We find the asymptotic covariance matrix for our estimate using the information $I(\beta)$, and we provide a 95% confidence interval for each of $\beta_0$ and $\beta_1$.

```{r, echo = FALSE}
x <- c(6.2, 4.2, 0.5,  8.8, 1.5, 9.2, 8.5, 8.7, 6.7, 6.5, 6.3, 6.7,  0.2, 8.7, 7.5)
y <- c(0.8, 3.5, 12.4, 1.1, 8.9, 2.4, 0.1, 0.5, 3.5, 8.3, 2.6, 1.5, 16.6, 0.1, 1.4)

knitr::kable(dplyr::tibble(x=x, y=y))

```

```{r, include = FALSE}

x <- c(6.2, 4.2, 0.5,  8.8, 1.5, 9.2, 8.5, 8.7, 6.7, 6.5, 6.3, 6.7,  0.2, 8.7, 7.5)
y <- c(0.8, 3.5, 12.4, 1.1, 8.9, 2.4, 0.1, 0.5, 3.5, 8.3, 2.6, 1.5, 16.6, 0.1, 1.4)

#\sum_i^n [\beta_0 + \beta_i x_i-e^{\beta_0+\beta_1 x_i y}]

loglike <- function(betas){
  sum(betas[1] + betas[2] * x) - sum(y*exp((betas[1] + betas[2] * x)))
}
my_optim <- optim(par = c(1,1), fn = loglike, control = list(fnscale = -1))

b0_hat <- my_optim$par[1]
b1_hat <- my_optim$par[2]

FIM <- matrix(c(length(x), sum(x), sum(x), sum(x^2)), ncol = 2, nrow = 2)
varcovar <- solve(FIM)

b0_ci <- matrix(c(b0_hat - 1.96 * sqrt(varcovar[1,1]), 
                  b0_hat + 1.96 * sqrt(varcovar[1,1])), nrow=1)

b1_ci <- matrix(c(b1_hat - 1.96 * sqrt(varcovar[2,2]), 
                  b1_hat + 1.96 * sqrt(varcovar[2,2])), nrow=1)

```  

The optimal values, based on `param` are $\hat{\beta_0} = $ `r round(b0_hat,3)` and $\hat{\beta_1} = $ `r round(b1_hat,3)`.  

The asymptotic distribution of the MLE is  
$$
\sqrt{n}(\boldsymbol{\hat\beta} - \boldsymbol{\beta}) \to_dN[\boldsymbol{0}, \boldsymbol{I(\boldsymbol{\beta})^{-1}}] 
$$  
And thus the covariance matrix is $\boldsymbol{I(\boldsymbol{\beta})^{-1}}$  

$$
\begin{bmatrix}
n & \sum_{i=1}^n x_i \\
\sum_{i=1}^n x_i  & \sum_{i=1}^n x_i^2
\end{bmatrix} ^ {-1} =
$$  

$$
\begin{bmatrix}
15 & 90.2 \\
90.2  & 672.3
\end{bmatrix} ^ {-1} =
$$  

$$
\begin{bmatrix}
0.345 & -0.046 \\
-0.046  & 0.008
\end{bmatrix}
$$  
This allows us to construct confidence intervals as follows:  
  
$$
\begin{bmatrix}
\hat{\beta_0}-1.96 \times \sqrt{\boldsymbol{I(\boldsymbol{\beta})_{[1,1]}^{-1}}} & \hat{\beta_0}+1.96 \times \sqrt{\boldsymbol{I(\boldsymbol{\beta})_{[1,1]}^{-1}}}
\end{bmatrix} = 
$$  

$$
\begin{bmatrix}
\hat{\beta_0}-1.96 \times \sqrt{0.345} & \hat{\beta_0}+1.96 \times \sqrt{0.345}
\end{bmatrix} =
$$  

$$
\begin{bmatrix}
-3.954 & -1.651
\end{bmatrix}
$$  
and  

$$
\begin{bmatrix}
\hat{\beta_1}-1.96 \times \sqrt{\boldsymbol{I(\boldsymbol{\beta})_{[2,2]}^{-1}}} & \hat{\beta_1}+1.96 \times \sqrt{\boldsymbol{I(\boldsymbol{\beta})_{[2,2]}^{-1}}}
\end{bmatrix} =
$$  
$$
\begin{bmatrix}
\hat{\beta_1}-1.96 \times \sqrt{0.0077} & \hat{\beta_1}+1.96 \times \sqrt{0.0077}
\end{bmatrix} = \\ 
$$  


$$
\begin{bmatrix}
0.125 & 0.469
\end{bmatrix}
$$  

## (d)  
Next, we plot the log-likelihood function $l(\boldsymbol{\beta})$. 

```{r, include = FALSE}
library(plotly)

b0 = seq(-5, -1, length.out = 10)
b1 = seq(0.1, 0.6, length.out = 10)

grid = expand.grid(b0, b1)
z <- NULL

for(i in 1:length(grid[,1])) {
  z = cbind(z, loglike(c(grid[i, 1], grid[1, 2])))
}

z_mat <- matrix(data = z, nrow = length(b0), ncol =length(b1))
fig <- plot_ly() %>% add_surface(x = b1, y = b0, z = z_mat, type = "mesh3d")
``` 

```{r, echo = FALSE, warning=FALSE}
fig
```  


## (e)  
We now find the maximum likelihood estimate for $\hat{\beta_0}$ under the null hypothesis $H_0:\beta_1=0$  

Given $H_0:\beta_1=0$ the log-likelihood $l(\boldsymbol{\beta})$ is now:  

$$
\sum_{i=1}^n\beta_0-\sum_{i=1}^ny_i(exp(\beta_0)) = n - exp(\beta_0)\sum_{i=1}^ny_i
$$  
Solving for $\beta_0$  

$$
\hat{\beta_0} = ln\biggl(\frac{n}{\sum_{i=1}^nyi}\biggl) = ln\bigg(\frac{15}{63.7}\bigg) = -1.4461
$$

```{r, echo = FALSE}


```  


## (f)  

```{r, include = FALSE}
b0_null <- log(15/63.7)
I <- FIM
S <- matrix(c(15 - sum(y * exp(b0_null)), 
              sum(x) - sum(y * x * exp(b0_null))), ncol = 2, nrow = 1)
score <- S %*% solve(I) %*% t(S)
pval_score <- pchisq(score, 1, lower.tail = F)

# wald
I_112 <- I[1,1] - I[1,2]*(I[2,2])^(-1)*I[2,1]
wald_stat <- b1_hat^2 * I_112
pval_wald <- pchisq(wald_stat, 1, lower.tail = FALSE)

# likelihood ratio 
likeli_stat <- 2 * (loglike(c(b0_hat,b1_hat)) - loglike(c(b0_null, 0)))
pval_likeratio <- pchisq(likeli_stat, 1, lower.tail = FALSE)

```

We finally perform a score, likelihood ratio, and Wald tests of the null hypothesis $H_0:\beta_1=0$ with $\alpha = 0.05$, indicating the formula we used to compute the test statistic.  

To perform a score test, we make use of the score statistics from (f). Our null hypothesis is then $H_0: \beta_1 = 0$. Then  

$$  
S([\hat{\beta_0},0])^TI^{-1}([\hat{\beta_0}, 0])S([\hat{\beta_0},0])
$$  
Where $I$ is the Information Matrix. We have computed both, the score function and the information matrix. It's just a matter of substituting quantities with $\hat{\beta_0} = -1.14461$.  Doing so results in `r round(score,2)`.  

The test statistic converges in distribution to a Chi-squared distribution with one degree of freedom, which allows to compute pvalue (`r round(pval_score, 3)`). Thus we reject the null hypothesis that $\beta_1 = 0$.  

Next, we do a ratio test. This test takes the form:  

$$
2\bigg[ l(\hat\beta_1)-l(\hat{\beta^0})  \bigg] 
$$  
This also converges in distribution to chi-square. Obtaining a pvalue from the likelihood ratio test yields `r round(pval_likeratio, 3)` and so we reject the null hypothesis that $\beta_1=0$.  

We now turn our attention to a Wald test of $\beta_1 = 0$. 

$$
\text{Wald Statistic} = \sqrt{n}[\hat{\beta_1}-\beta_1^0]^TI_{11*2}(\boldsymbol{\hat{\beta}})\sqrt{n}[\sqrt{n}[\hat{\beta_1}-\beta_1^0]] \to_d \mathcal{X}^2_r
$$  
And this also converges in distribution to the Chi-square distribution. Computing the Wald Statistic and its pvalue under a chi-square, we get: `r round(pval_wald,2)`, and thus we fail to reject the null hypothesis that $\beta_1=0$ under Wald test.  

## (g)  
Based on the hypothesis testing of the score, likelihood ratio, and Wald statistics, (which all but Wald rejected the null that $\beta_1=0$), we conclude that as the concentrations of a contaminant vary the survival of the rats also varies. In other words, there seems to be an association between the amount of contaminant and a rat's survival time.  



## Appendix  


```{r, eval=FALSE}
# student: Raul Torres Aragon
# date: 2022-10-13
# assignment: 570 hw 2
# notes:



#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# QUESTION 1
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

rm(list = ls())
set.seed(570)
b0 <- 3
b1 <- -3
Nsim <- 1000

# create function to do simulations 

run_sims <- function(Nsim = Nsim, b0=3, b1=-3, n=15) {
  
 x <- rnorm(mean=20, sd=2, n=n)

 ModName      <- vector(mode = "character", length = n*3)
 Biases       <- vector(mode = "numeric", length = n*3)
 Var_beta0    <- vector(mode = "numeric", length = n*3)
 Var_beta1    <- vector(mode = "numeric", length = n*3)
 Beta0s       <- vector(mode = "numeric", length = n*3)
 Beta1s       <- vector(mode = "numeric", length = n*3)
 CI_beta0_low <- vector(mode = "numeric", length = n*3) 
 CI_beta0_hig <- vector(mode = "numeric", length = n*3)
 CI_beta1_low <- vector(mode = "numeric", length = n*3)
 CI_beta1_hig <- vector(mode = "numeric", length = n*3)
 
 ynames <- c("Y_norm","Y_unif","Y_skew")
 j = 0
 for(i in 1:Nsim) {
   
   y_norm <- b0 + b1*x + rnorm(mean=0, sd=2, n=n)
   y_unif <- b0 + b1*x + runif(min=-5, max=5, n=n)
   y_skew <- b0 + b1*x + sn::rsn(alpha=5, omega=1, xi=-5/(sqrt(13*pi)), n=n)[1:n]
   
   ys <- list(y_norm, y_unif, y_skew)
   
   k = 0
   for(y in ys){
     k = k+1
     j = j+1
     mymod <- lm(y~x)
     yhat  <- predict(mymod)
     bias  <- mean(yhat - y)
     CIs   <- confint(mymod, param = c(0,1), level = 0.80)
     
     ModName[j]   <- paste(ynames[k], i)
     Biases[j]    <- bias
     Var_beta0[j] <- vcov(mymod)[1,1]
     Var_beta1[j] <- vcov(mymod)[2,2]
     Beta0s[j]    <- mymod$coefficients[[1]]
     Beta1s[j]    <- mymod$coefficients[[2]]
     CI_beta0_low[j] <- CIs[1]
     CI_beta1_low[j] <- CIs[2]
     CI_beta0_hig[j] <- CIs[3]
     CI_beta1_hig[j] <- CIs[4]    
     
   }
 }
 
 results <- dplyr::tibble(model = ModName, 
                          bias = Biases,
                          var_beta0 = Var_beta0,
                          var_beta1 = Var_beta1,
                          b0 = Beta0s,
                          b1 = Beta1s,
                          b0_ci_low = CI_beta0_low,
                          b0_ci_hig = CI_beta0_hig,
                          b1_ci_low = CI_beta1_low,
                          b1_ci_hig = CI_beta1_hig                         
 )
 results$type <- stringr::str_sub(results$model, 3, 6)
 results$b0_inside <- 0
 results$b0_inside[results$b0_ci_low < b0 & b0 < results$b0_ci_hig] <- 1
 results$b1_inside <- 0
 results$b1_inside[results$b1_ci_low < b1 & b1 < results$b1_ci_hig] <- 1
 
 
 # confirm numerically that the bias is zero
 bias_results <- sum(results$bias != 0)

 # compare variance of betas with the sampling distribution of betas_hat
 sampl_var_b0_norm <- results$b0[results$type == "norm"] |> var() 
 sampl_var_b1_norm <- results$b1[results$type == "norm"] |> var() 
 
 sampl_var_b0_unif <- results$b0[results$type == "unif"] |> var() 
 sampl_var_b1_unif <- results$b1[results$type == "unif"] |> var() 
 
 sampl_var_b0_skew <- results$b0[results$type == "skew"] |> var() 
 sampl_var_b1_skew <- results$b1[results$type == "skew"] |> var() 
 
 mean_varb0_norm <- results$var_beta0[results$type == "norm"] |> mean() 
 mean_varb0_unif <- results$var_beta0[results$type == "unif"] |> mean() 
 mean_varb0_skew <- results$var_beta0[results$type == "skew"] |> mean() 
 
 mean_varb1_norm <- results$var_beta1[results$type == "norm"] |> mean() 
 mean_varb1_unif <- results$var_beta1[results$type == "unif"] |> mean() 
 mean_varb1_skew <- results$var_beta1[results$type == "skew"] |> mean() 
 
 # examine the distribution of the resultant estimators (across sims) of betas...
 ci_b0_norm_inside <- mean(results[results$type == "norm", ]$b0_inside)
 ci_b0_unif_inside <- mean(results[results$type == "unif", ]$b0_inside)
 ci_b0_skew_inside <- mean(results[results$type == "skew", ]$b0_inside)
 ci_b1_norm_inside <- mean(results[results$type == "norm", ]$b1_inside)
 ci_b1_unif_inside <- mean(results[results$type == "unif", ]$b1_inside)
 ci_b1_skew_inside <- mean(results[results$type == "skew", ]$b1_inside)
 
 
 o <- list("bias_results"      = bias_results, 
           "ci_b0_norm_inside" = ci_b0_norm_inside,
           "ci_b0_unif_inside" = ci_b0_unif_inside,
           "ci_b0_skew_inside" = ci_b0_skew_inside,
           "ci_b1_norm_inside" = ci_b1_norm_inside,
           "ci_b1_unif_inside" = ci_b1_unif_inside,
           "ci_b1_skew_inside" = ci_b1_skew_inside,
           "sampl_var_b0_norm" = sampl_var_b0_norm,
           "sampl_var_b1_norm" = sampl_var_b1_norm,
           "sampl_var_b0_unif" = sampl_var_b0_unif,
           "sampl_var_b1_unif" = sampl_var_b1_unif,
           "sampl_var_b0_skew" = sampl_var_b0_skew,
           "sampl_var_b1_skew" = sampl_var_b1_skew,
           "mean_varb0_norm"   = mean_varb0_norm,
           "mean_varb0_unif"   = mean_varb0_unif,
           "mean_varb0_skew"   = mean_varb0_skew,
           "mean_varb1_norm"   = mean_varb1_norm,
           "mean_varb1_unif"   = mean_varb1_unif,
           "mean_varb1_skew"   = mean_varb1_skew,           
           "results_df"        = results
           )
 
 return(o)
 
} # end function

o15 <- run_sims(Nsim = Nsim, n = 15)
o40 <- run_sims(Nsim = Nsim, n = 40)


### (b)
par(mfrow = c(1,3))

hist(o15$results_df$b1[o15$results_df$type == "norm"], main = "b1 normal epsilon", 
     xlab = paste0("mean(lm var) = ", 
                   round(o15$mean_varb1_norm,2), "\n sample var =", 
                   round(o15$sampl_var_b1_norm,2)))

hist(o15$results_df$b1[o15$results_df$type == "unif"], main = "b1 normal epsilon", 
     xlab = paste0("mean(lm var) = ", 
                   round(o15$mean_varb1_unif,2), "\n sample var =", 
                   round(o15$sampl_var_b1_unif,2)))

hist(o15$results_df$b1[o15$results_df$type == "skew"], main = "b1 normal epsilon", 
     xlab = paste0("mean(lm var) = ", 
                   round(o15$mean_varb1_skew,2), "\n sample var =", 
                   round(o15$sampl_var_b1_skew,2)))


### (c)
library(plotrix)

sn <- 10

samp <- sample(1:1000, size = sn)

plotrix::plotCI(x=1:sn, rep(-3,sn), 
                abs(o15$results_df$b0_ci_low[o15$results_df$type == "norm"][samp]), 
                abs(o15$results_df$b0_ci_hig[o15$results_df$type == "norm"][samp]), 
                main = "100 randomly selected 10% CI for b1", ylab = "", xlab = "")

plotrix::plotCI(x=1:sn, rep(-3,sn), 
                abs(o15$results_df$b1_ci_low[o15$results_df$type == "norm"][samp]), 
                abs(o15$results_df$b1_ci_hig[o15$results_df$type == "norm"][samp]), 
                main = "100 randomly selected 10% CI for b1", ylab = "", xlab = "")


# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# QUESTION 2
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
rm(list = ls())

x <- c(6.2, 4.2, 0.5,  8.8, 1.5, 9.2, 8.5, 8.7, 
       6.7, 6.5, 6.3, 6.7,  0.2, 8.7, 7.5)

y <- c(0.8, 3.5, 12.4, 1.1, 8.9, 2.4, 0.1, 0.5, 
       3.5, 8.3, 2.6, 1.5, 16.6, 0.1, 1.4)

loglike <- function(betas){
  sum(betas[1] + betas[2] * x) - sum(y*exp((betas[1] + betas[2] * x)))
}
my_optim <- optim(par = c(1,1), fn = loglike, control = list(fnscale = -1))

b0_hat <- my_optim$par[1]
b1_hat <- my_optim$par[2]

FIM <- matrix(c(length(x), sum(x), sum(x), sum(x^2)), ncol = 2, nrow = 2)
varcovar <- solve(FIM)

b0_ci <- matrix(c(b0_hat - 1.96 * sqrt(varcovar[1,1]), 
                  b0_hat + 1.96 * sqrt(varcovar[1,1])), nrow=1)

b1_ci <- matrix(c(b1_hat - 1.96 * sqrt(varcovar[2,2]), 
                  b1_hat + 1.96 * sqrt(varcovar[2,2])), nrow=1)



# (d) Plot
library(plotly)

b0 <- seq(-5, -1, length.out = 10)
b1 <- seq(0.1, 0.6, length.out = 10)

grid <- expand.grid(b0, b1)
z <- NULL

for(i in 1:length(grid[,1])) {
  z <- cbind(z, loglike(c(grid[i, 1], grid[1, 2])))
}

z_mat <- matrix(data = z, nrow = length(b0), ncol =length(b1))
fig <- plot_ly() %>% add_surface(x = b1, y = b0, z = z_mat, type = "mesh3d")
fig


# (f)
b0_null <- log(15/63.7)
I <- FIM
S <- matrix(c(15 - sum(y * exp(b0_null)), 
              sum(x) - sum(y * x * exp(b0_null))), ncol = 2, nrow = 1)
score <- S %*% solve(I) %*% t(S)
pval_score <- pchisq(score, 1, lower.tail = F)

# wald
I_112 <- I[1,1] - I[1,2]*(I[2,2])^(-1)*I[2,1]
wald_stat <- b1_hat^2 * I_112
pval_wald <- pchisq(wald_stat, 1, lower.tail = FALSE)

# likelihood ratio 
likeli_stat <- 2 * (loglike(c(b0_hat,b1_hat)) - loglike(c(b0_null, 0)))
pval_likeratio <- pchisq(likeli_stat, 1, lower.tail = FALSE)

```




















