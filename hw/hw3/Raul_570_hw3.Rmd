---
title: "<center> <h1>Homework 3</h1> </center>"
author: "Raul Torres Aragon"
date: "10/18/2022"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(MASS)
options(scipen=999)
load('hw3_environment_final.RData')
```

## 1
Considering the Poisson-gamma random effects model given by:  
$$
Y_i | \mu_i \sim \text{Poisson}(\mu_i\theta_i) \\
\theta_i \sim \text{Gamma}(b,b)
$$  

which leads to a negative binomial marginal model with the variance a qudratic function of the mean. We design a simulation to investigate the efficiency and robustness under the following:  

* Poisson model  
* A negative binomial model  
* Quasi-likelihood with $E[Y] = \mu$, $var(Y) = \alpha\mu$  
* Sandwich estimation  

and using a loglinear model $log(\mu_i) = \beta_0 + \beta_1x_i$.  

For this simulation we iterate over the following $b$ values: 0.1, 1, 10, and 1000.  
We also iterate over the following $n$ values: 10, 20, 50, 100, 250.  

Each simulation is run 1,000 times. 
The following table shows the results for $\beta_0$.  

```{r, echo = FALSE}
knitr::kable(b0_table)


```  

This table shows results for $\beta_1$  

```{r, echo = FALSE}
knitr::kable(b1_table)

```

Notice how negative binomial is not defined when $b$ (the dispersion) parameter is 0.1, but when it is defined, its coverage is consistently above 90%. The Poisson model catches up to the binomial when $b$ is large. Poisson estimates with quasi-likelihood standard errors improves the coverage, even in the presence of over overdispersion. Sandwich standard errors (with the Poisson estimates) do not have great coverage regardless of overdispersion and even when $n$ is large.  


## 2  

We now turn our attention to the attached table of stress data for four groups.  
The exponential distribution $Y|\lambda \sim_{iid} \text{Exp}(\lambda)$ with $\lambda, y >0$. The hazard function  
$$
h(y|\lambda) = \frac{p(y|\lambda)}{S(y|\lambda)}  
$$  
where $S(y|\lambda) = Pr(Y>y|\lambda)$ is the probability of failure beyond $y$.  

### (a)  
And it is derived from this model as follows:  
$$
p(y|\lambda) = \lambda\exp\{-\lambda y\}  \\
h(y|\lambda) = \frac{\lambda\exp\{-\lambda y \}}{Pr(Y>y|\lambda)} = \\
\frac{\lambda\exp\{-\lambda y \}}{1 - F(Y)} = \\ 
\frac{\lambda\exp\{-\lambda y \}}{1-(1-\exp\{-\lambda y\})} = 
\lambda
$$  
We also derive the MLE of $Y$.  

$$
L(\lambda) = \prod_i^n \lambda\exp\{-\lambda y \} = \lambda^ne^{-\lambda \sum_i^ny_i} \\
l(\lambda) = log(L(\lambda)) = nlog(\lambda) - \lambda \sum_{i=1}^n y_i \\
l'(\lambda) = Sc(\lambda) = \frac{n}{\lambda} - \sum_{i=1}^n y_i  \\
\hat{\lambda}_{MLE} = \frac{n}{\sum_{i=1}^n y_i}  \\
I(\lambda) = -E\bigg[ \frac{\partial^2l(\lambda)}{\partial \lambda}\bigg] = \frac{n}{\lambda^2}  
$$  
And therefore the asymptotic variance is $[I(\lambda)]^{-1} = \frac{\lambda^2}{n}$.  

### (b)  
Using the data from the the attached table, we estimate a separate $\lambda$ per group, and examine the appropriateness of the fit with QQ-plots.  

We find that for group 1 $\hat{lambda}_{g1}$ = `r round(g1_stuff$lambda, 2)`; for group 2 $\hat{lambda}_{g2}$ = `r round(g2_stuff$lambda, 2)`; for group 3 $\hat{lambda}_{g3}$ = `r round(g3_stuff$lambda, 2)`; and for group 4 $\hat{lambda}_{g4}$ = `r round(g4_stuff$lambda, 2)`.  
The associated standard errors, which we get by taking the square root of the variance are for group 1  = `r round(g1_stuff$s, 3)`; for group 2 = `r round(g2_stuff$s, 3)`; for group 3 = `r round(g3_stuff$s, 3)`; and for group 4 = `r round(g4_stuff$s, 3)`.  

The following QQ plots show the normality of the residuals, that is, when we take the difference between the observed failure stress levels and the predicted with $\hat{\lambda}_{g_i}$.  

```{r, echo = FALSE}
par(mfrow=c(2,2))
for(i in 1:4) {
qqnorm(gps[[i]] - lambdas[[i]], pch = 1, 
       ylab = paste0("Group ", i, " - MLE(", i,")"), 
       main = paste0("Group ", i))
qqline(gps[[i]] - lambdas[[i]], col = "darkblue", lwd =2)
}

```  

The residuals are roughly normally distributed, at least for group 1.  

### (c)  

We now consider a quasi-likelihood approach to inference with $E[Y] = \lambda^{-1}$ and $var(Y|\lambda) = \alpha \lambda^{-2}$ with $\alpha > 0$.  

Assuming the variance is correct, the the Pearson statistic divided by its degrees of freedom yields a (not always unbiased but) consistent estimate of $\alpha$.  

$$
\hat{\alpha} = \frac{1}{n-k-1} \sum_{i=1}^n \frac{(y_i - \lambda^{-1})^2}{V(\lambda^{-1})}
$$  
where $Var(\lambda) = \frac{1}{\lambda^2}$ because we assume $Y\sim\text{Exp}(\lambda)$. Thus  
$$
\hat{\alpha} = \frac{1}{n-2}\lambda^2\sum_{i=1}^n(y_i-\lambda^{-1})^2
$$  
Having estimated $\alpha$ we get standard errors for each group. We find the associated standard errors are for group 1  = `r round(g1_stuff$s_quas, 3)`; for group 2 = `r round(g2_stuff$s_quas, 3)`; for group 3 = `r round(g3_stuff$s_quas, 3)`; and for group 4 = `r round(g4_stuff$s_quas, 3)`. In other words, it seems that the quasilikelihood standard errors produce narrower confidence intervals, giving us a better handle on the overdispersion in the variance of Y.  

### (d)  

We now obtain the form of a sandwich estimator as follows:  

$$
G(\hat{\lambda}, y_i) = \frac{1}{n}\sum_{i=1}^n\frac{\partial l(\lambda)}{\partial \lambda} = \frac{1}{n} \sum_{i=1}^n\bigg[ \frac{n}{\lambda} - \sum_{i=1}^n y_i \bigg] = \frac{1}{n}\bigg[n\frac{n}{\lambda} - n\sum_{i=1}^n y_i \bigg] = \frac{n}{\lambda} - \sum_{i=1}^n y_i  
$$  

We then define A and B as follows:  

$$
A = E\bigg[\frac{\partial}{\partial \lambda} (\frac{n}{\lambda} - \sum_{i=1}^n y_i)\bigg] = E\bigg[-\frac{n}{\lambda^2} \bigg] = -\frac{n}{\lambda^2} \\  
B = \frac{1}{n} \sum_{i=1}^n G(\lambda)^2 = \frac{1}{n}\sum_{i=1}^n\bigg(\frac{n}{\lambda} - \sum_{i=1}^n y_i \bigg)  \\
var(\hat{\lambda}) = \frac{A^{-1}B(A^T)^{-1}}{n}
$$  
The above equation results in this new estimate for the variance of each $\hat{\lambda}_{g_i}$. We find the associated standard errors are for group 1  = `r round(g1_stuff$s_sand, 3)`; for group 2 = `r round(g2_stuff$s_sand, 3)`; for group 3 = `r round(g3_stuff$s_sand, 3)`; and for group 4 = `r round(g4_stuff$s_sand, 3)`.  

### (e)  

We turn our attention to the Weibull distribution and later model.  
We have that the pdf, mean, variance, and hazard functions are:  

$$
Y|\eta,\alpha = \frac{\eta}{\alpha} y^{\eta-1} e^{-\frac{y^\eta}{\alpha}}
$$

$$
E(Y) = \int_0^\infty y \frac{\eta}{\alpha} y^{\eta-1} e^{-\frac{y^\eta}{\alpha}} dy = \frac{\eta}{\alpha} \int_0^\infty y^{n-1}e^{\frac{y^\eta}{\alpha}}dy  
$$  
Now, define the gamma function as $\Gamma(\alpha) = \int_0^\infty t^{\alpha - 1} e^{-t} dt$ by substitution, letting $t = \frac{y^n}{\alpha}$ and $y=(\alpha t)^{\frac{1}{\eta}}$ one can show that  

$$
E(Y) = \alpha^{\frac{1}{\eta}} \Gamma(1+\frac{1}{\eta}) 
$$  

Then, repeating the process but with $y^2$ one obtains  

$$
E(Y^2) = \alpha^{\frac{2}{\eta}} \Gamma(1+\frac{2}{\eta})  
$$  

This allows us to compute the variance as  
$$
E(Y^2) - E(Y)^2 = Var(Y) = \alpha^{\frac{2}{\eta}} \Gamma(1+\frac{2}{\eta})(1-\Gamma(1+\frac{2}{\eta}))) 
$$  

Lastly, the hazard function, which comes from observing the cumulative distribution function (in closed form) for the WEibull distribution is  
$$
h(Y|\eta,\alpha) = \frac{\eta \alpha^{-\eta} y^{\eta-1}e^{-(\frac{y}{\alpha})^{\eta}}}{e^{-(\frac{y}{\alpha})^{\eta}}} = \eta\alpha^{-\eta}y^{\eta-1}
$$  

Notice that the Weibull distribution is defined for parameter values in $(0, \infty)$.  

### (f)  

This distribution is part of the exponential family. This means that it is of the form $f(x) = h(x) \exp\{\eta(\theta) T(x) + A(\theta)\}$ which in turn means $T(x)$ is a minimally sufficient statistic. Likelihood functions flow well from exponentially family and MLEs are easy to find explicitly, which is good for estimation. But most importantly, their parameters are asymptotically normal, which means that as sample size grows large they become normally distributed.  

### (g)  
We now obtain its likelihood function.  

$$
L(\eta, \alpha) = \prod_{i=1}^n 
 \eta \alpha^{-\eta}y^{\eta-1}\exp\{-(\frac{y}{\alpha})^{\eta}\} = 
(\eta \alpha^{-\eta})^{n} (\prod_i^n y_i)^{\eta-1} \exp\{-\sum_i^n(\frac{y}{\alpha})^{\eta}\}
$$

$$
l(\eta, \alpha) = nlog(\eta\alpha^{-\eta}) + (\eta-1)\sum_i^nlog(y_i) - \sum_i^n(\frac{y_i}{\alpha})^{\eta}
$$
$$
\frac{\partial l(\eta, \alpha)}{\partial \eta} = \frac{n}{\eta} - nlog(\alpha)-\frac{\sum_i^ny_i^\eta - log(\alpha)\sum_i^n y_i^\eta}{\alpha^\eta}:=0
$$

$$
I(\eta, \alpha) = -E\bigg[\frac{\partial l(\eta, \alpha)}{\partial \eta \partial \alpha}\bigg] = \\
-E\begin{bmatrix} 
\frac{\partial l(\eta, \alpha)}{\partial \eta \partial \eta} & \frac{\partial l(\eta, \alpha)}{\partial \eta \partial \alpha} \\ 
\frac{\partial l(\eta, \alpha)}{\partial \alpha \partial \eta} & \frac{\partial l(\eta, \alpha)}{\partial \alpha \partial \alpha} \\ 
\end{bmatrix} = \\
I_{[1,1]} = \alpha^\eta log(\alpha)(\sum_i^n-log(\alpha))+\alpha^{-y_i} log(\alpha)y_i^\eta log(y_i) - \frac{\eta}{y_i^2} \\
I_{[1,2]} = I_{[2,1]} = \\\alpha^{-eta -1} (-\eta\alpha^{y_i}+\eta(\sum_i^ny_i-ln(\alpha)\sum_i^ny_i^\eta)+\sum_i^ny_i^{\eta}) \\
I_{[2,2]} = \eta\alpha^{-eta-2}(n\alpha^{y_i}-\sum_i^ny_i-1)
$$

### (h)  
The two equations that we get cannot be solved simultaneously analytically. We can solve for $\eta$ and substituting it in to obtain $\alpha$. But obtaining $\eta$ can only be done analytically.  
We first solve numerically for 
$$
\frac{1}{\eta} - \frac{\sum_i^ny_i^\eta log(y_i)}{\sum_i^n y_i^\eta} + \frac{1}{n}\sum_{i=1}^n log(y_i)
$$  

$$
\alpha = \bigg[\frac{1}{n}\sum_i^n y_i^\eta\bigg]^{\frac{1}{n}}
$$


### (i)  
We solve for its values using `optim` ans the values we get are:  
for group 1  alpha = `r round(g1_stuff$weibul$alpha, 3)` and for $\eta$ = `r round(g1_stuff$weibul$eta, 3)`; 
for group 2 = alpha = `r round(g2_stuff$weibul$alpha, 3)` and for $\eta$ = `r round(g2_stuff$weibul$eta, 3)`; 
for group 3 = alpha = `r round(g3_stuff$weibul$alpha, 3)` and for $\eta$ = `r round(g3_stuff$weibul$eta, 3)`; and 
for group 4 = alpha = `r round(g4_stuff$weibul$alpha, 3)` and for $\eta$ = `r round(g4_stuff$weibul$eta, 3)`.  


## Appendix
```{}
#~~~~~~~~~~~~~~~~~~~~~~~
# student: Raul Aragon
# class: STAT 570
# assignment: hw3
# notes:
#~~~~~~~~~~~~~~~~~~~~~~~

library(MASS)
library(stringr)
library(tidyverse)
library(tibble)
library(dplyr)
rm(list=ls())
set.seed(570)

#~~~~~~~~~~~~#
# Question 1 #
#~~~~~~~~~~~~#


# HELPER FUNCTIONS
# ----------------

# negative binomial estimator
my_nbin <- function(X, Y, b0, b1) {
  mod <- try(glm.nb(Y~X))
  if(class(mod) == "try-error") { 
    b0_inside <- NA
    b1_inside <- NA
  } else {
    mat <- coef(summary(mod))
    CIs <- confint.default(mod)
    b0_inside <- (CIs[1,1] < b0 & b0 < CIs[1,2]) |> as.numeric()
    b1_inside <- (CIs[2,1] < b1 & b1 < CIs[2,2]) |> as.numeric()
  }
  return(list("b0_in_nbin" = b0_inside,
              "b1_in_nbin" = b1_inside))

}

# poisson estimator using MASS::glm
my_pois <- function(X, Y, b0, b1) {
  mod <- try(glm(Y~X, family = poisson))
  if(class(mod) == "try-error") { 
    b0_inside <- NA
    b1_inside <- NA
  } else {
  mat <- coef(summary(mod))
  CIs <- confint.default(mod)
  b0_inside <- (CIs[1,1] < b0 & b0 < CIs[1,2]) |> as.numeric()
  b1_inside <- (CIs[2,1] < b1 & b1 < CIs[2,2]) |> as.numeric()
  }
  return(list("b0_in_pois" = b0_inside,
              "b1_in_pois" = b1_inside))  
}

# poisson quasi-likelihood using MASS::glm
my_quas <- function(X, Y, b0, b1) {
  mod <- glm(Y~X, family = quasipoisson) |> try()
  if(class(mod) == "try-error") { 
    b0_inside <- NA
    b1_inside <- NA
  } else {  
  mat <- coef(summary(mod))
  CIs <- confint.default(mod)
  b0_inside <- (CIs[1,1] < b0 & b0 < CIs[1,2]) |> as.numeric()
  b1_inside <- (CIs[2,1] < b1 & b1 < CIs[2,2]) |> as.numeric()
  }
  return(list("b0_in_quas" = b0_inside,
              "b1_in_quas" = b1_inside))  
}

# sandwich estimator using sandwich::sandwich
my_sand <- function(X, Y, b0, b1) {
  mod <- glm(Y~X, family = poisson) |> try()
  if(class(mod) == "try-error") { 
    b0_inside <- NA
    b1_inside <- NA
  } else {  
  sand <- sandwich::sandwich(mod)
  
  b0_inside <- ((mod$coefficients[[1]] - sqrt(sand[1,1])) < b0 &
                 b0 < mod$coefficients[[1]] + sqrt(sand[1,1])) |> as.numeric()
  b1_inside <- ((mod$coefficients[[2]] - sqrt(sand[2,2])) < b1 &
                 b1 < mod$coefficients[[2]] + sqrt(sand[2,2])) |> as.numeric()
  }
  return(list("b0_in_sand" = b0_inside,
              "b1_in_sand" = b1_inside))  
  
}
  
  

# simulations function
# --------------------

run_sims <- function(n, b, b0=0.5, b1=log(2.5), N = 1000) {
  
  b0_nbin_inside <- rep(NA, N) #vector(mode = "numeric", length = N)
  b1_nbin_inside <- rep(NA, N) #vector(mode = "numeric", length = N)
  b0_pois_inside <- rep(NA, N) #vector(mode = "numeric", length = N)
  b1_pois_inside <- rep(NA, N) #vector(mode = "numeric", length = N)
  b0_quas_inside <- rep(NA, N) #vector(mode = "numeric", length = N)
  b1_quas_inside <- rep(NA, N) #vector(mode = "numeric", length = N)
  b0_sand_inside <- rep(NA, N) #vector(mode = "numeric", length = N)
  b1_sand_inside <- rep(NA, N) #vector(mode = "numeric", length = N)
  
  j = 0
  for(i in 1:N) {
    j = j+1
    
    X <- rnorm(mean=0, sd=1, n=n)
    theta <- rgamma(shape=b, rate=b, n=n)
    mu <- exp(b0 + b1*X)
    Y <- rpois(lambda = theta*mu, n=n) 
    
    # negative binomial
    mod_nbin <- my_nbin(Y=Y, X=X, b0=b0, b1=b1) 
    b0_nbin_inside[j] <- mod_nbin[[1]]
    b1_nbin_inside[j] <- mod_nbin[[2]]
  
    # poisson
    mod_pois <- my_pois(Y=Y, X=X, b0=b0, b1=b1)
    b0_pois_inside[j] <- mod_pois[[1]]
    b1_pois_inside[j] <- mod_pois[[2]]
      
    # quasi-likelihood  
    mod_quas <- my_quas(Y=Y, X=X, b0=b0, b1=b1)
    b0_quas_inside[j] <- mod_quas[[1]]
    b1_quas_inside[j] <- mod_quas[[2]]
      
    # sandwich estimator  
    mod_sand <- my_sand(Y=Y, X=X, b0=b0, b1=b1)
    b0_sand_inside[j] <- mod_sand[[1]]
    b1_sand_inside[j] <- mod_sand[[2]]
  }
  
  o <- list("b0_nbin_inside" = mean(b0_nbin_inside),
            "b1_nbin_inside" = mean(b1_nbin_inside),
            "b0_pois_inside" = mean(b0_pois_inside),
            "b1_pois_inside" = mean(b1_pois_inside),
            "b0_quas_inside" = mean(b0_quas_inside),
            "b1_quas_inside" = mean(b1_quas_inside),
            "b0_sand_inside" = mean(b0_sand_inside),
            "b1_sand_inside" = mean(b1_sand_inside),
            "b0_vector_nbin" = b0_nbin_inside,
            "b1_vector_nbin" = b1_nbin_inside,
            "b0_vector_pois" = b0_pois_inside,
            "b1_vector_nbin" = b1_pois_inside,
            "b0_vector_quas" = b0_quas_inside,
            "b1_vector_quas" = b1_quas_inside,
            "b0_vector_sand" = b0_sand_inside,
            "b1_vector_sand" = b1_sand_inside)
  return(o)
}

ns <- c(10,20,50,100,250)
bs <- c(1, 10, 1000, 0.1)

# invoke simulations function for each combination of ns and bs

b0_table <- tibble("n" = integer(), 
                   "b" = integer(), 
                   "nbinomial" = double(),
                   "poisson" = double(),
                   "quasi" = double(),
                   "sandwich" = double()) 

b1_table <- tibble("n" = integer(), 
                   "b" = integer(), 
                   "nbinomial" = double(),
                   "poisson" = double(),
                   "quasi" = double(),
                   "sandwich" = double())
for(b in bs) {
  for(n in ns) {
    print(paste0("for n=", n, " and b=", b))
    vals <- run_sims(n=n, b=b, b0=0.5, b1=log(2.5), N=1e4)
    b0_table <- b0_table |> 
                add_row("n" = n, "b" = b, 
                        "nbinomial" = vals[[1]], 
                        "poisson" = vals[[3]], 
                        "quasi" = vals[[5]], 
                        "sandwich" = vals[[7]])
    
    b1_table <- b1_table |> 
                add_row("n" = n, "b" = b, 
                        "nbinomial" = vals[[2]], 
                        "poisson" = vals[[4]], 
                        "quasi" = vals[[6]], 
                        "sandwich" = vals[[8]])
  }
}

b1_tab <- pivot_wider(b1_table, id_cols = n, 
                      names_from = b, 
                      values_from = nbinomial:sandwich
          ) |> dplyr::select(n, 
                       `nbinomial_0.1`, `poisson_0.1`, `quasi_0.1`, `sandwich_0.1`,
                        nbinomial_1,    poisson_1,      quasi_1,     sandwich_1,
                        nbinomial_10,   poisson_10,     quasi_10,    sandwich_10,
                        nbinomial_1000, poisson_1000,   quasi_1000,  sandwich_1000)

b0_tab <- pivot_wider(b0_table, id_cols = n, 
                      names_from = b, 
                      values_from = nbinomial:sandwich
          ) |> dplyr::select(n, 
                   `nbinomial_0.1`, `poisson_0.1`, `quasi_0.1`, `sandwich_0.1`,
                   nbinomial_1,    poisson_1,      quasi_1,     sandwich_1,
                   nbinomial_10,   poisson_10,     quasi_10,    sandwich_10,
                   nbinomial_1000, poisson_1000,   quasi_1000,  sandwich_1000)



#~~~~~~~~~~~~#
# Question 2 #
#~~~~~~~~~~~~#


# (b)

g1<-c(2.247,2.640,2.842,2.908,3.099,3.126,3.245,3.328,3.355,3.383,3.572,3.581,3.681)
g2<-c(1.901,2.132,2.203,2.228,2.257,2.350,2.361,2.396,2.397,2.445,2.454,2.454,2.474)
g3<-c(1.312,1.314,1.479,1.552,1.700,1.803,1.861,1.865,1.944,1.958,1.966,1.997,2.006)
g4<-c(1.339,1.434,1.549,1.574,1.589,1.613,1.746,1.753,1.764,1.807,1.812,1.840,1.852)

get_lambda <- function(g) {
  lambda <- length(g)/sum(g)
  var <- (1/length(g)) * (lambda^2)
  s <- sqrt(var)
  return(list("lambda" = lambda, "var" = var, "s" = s))
}

g1_stuff <- get_lambda(g1)
g2_stuff <- get_lambda(g2)
g3_stuff <- get_lambda(g3)
g4_stuff <- get_lambda(g4)

var_lamb1 <- lambda_1^2*(1/13)

# exponential qq-plot 
gps <- list(g1, g2, g3 ,g4)
lambdas <- list(g1_stuff$lambda, g2_stuff$lambda, g3_stuff$lambda, g4_stuff$lambda)

par(mfrow=c(2,2))
for(i in 1:4) {
qqnorm(gps[[i]] - lambdas[[i]], pch = 1, 
       ylab = paste0("Group ", i, " - MLE(", i,")"), 
       main = paste0("Group ", i))
qqline(gps[[i]] - lambdas[[i]], col = "darkblue", lwd =2)
}

# exponential QQ plots
### par(mfrow=c(2,2))
### pts <- ppoints(13)
### q <- quantile(g1, p = pts)
### plot(qexp(pts), q, 
###      main="Exponential Q-Q Plot",
###      xlab="Theoretical Quantiles",
###      ylab="Group 1 Quantiles")
### qqline(q, distribution=qexp,col="blue", lty=2)


# (c)
# estimator for \alpha

alphas <- vector(mode = "numeric", length = 4)
for(i in 1:4) {  
 alphas[i] <- (lambdas[[i]]^2/(length(gps[[i]])-2)) * sum((gps[[i]] - (lambdas[[i]])^-1)^2)
}
alphas

g1_stuff$var_quas <- alphas[[1]]*(lambdas[[1]])^2
g1_stuff$s_quas <- sqrt(alphas[[1]]*(lambdas[[1]])^2)

g2_stuff$var_quas <- alphas[[2]]*(lambdas[[2]])^2
g2_stuff$s_quas <- sqrt(alphas[[2]]*(lambdas[[2]])^2)

g3_stuff$var_quas <- alphas[[3]]*(lambdas[[3]])^2
g3_stuff$s_quas <- sqrt(alphas[[3]]*(lambdas[[3]])^2)

g4_stuff$var_quas <- alphas[[4]]*(lambdas[[4]])^2
g4_stuff$s_quas <- sqrt(alphas[[4]]*(lambdas[[4]])^2)

# (d)
# 

G <- function(Y, lambda) {
  length(Y)/lambda - sum(Y)
}

A <- function(Y, lambda) {
   -length(Y) / lambda^2
}

B <- function(Y, lambda) {
  (1/length(Y)) * sum((length(Y)/lambda - sum(Y))^2)
}


Var_sand <- function(Y, lambda) {
  lambda^4/length(Y)^2 * sum(((1/lambda) - Y)^2)
}

g1_stuff[["s_sand"]] <- sqrt(Var_sand(gps[[1]], lambdas[[1]]))
g2_stuff[["s_sand"]] <- sqrt(Var_sand(gps[[2]], lambdas[[2]]))
g3_stuff[["s_sand"]] <- sqrt(Var_sand(gps[[3]], lambdas[[3]]))
g4_stuff[["s_sand"]] <- sqrt(Var_sand(gps[[4]], lambdas[[4]]))

glimpse(g1_stuff)
glimpse(g2_stuff)
glimpse(g3_stuff)
glimpse(g4_stuff)


# (h)
alpha_fun <- function(Y, eta) {
  ((1/length(Y)) * sum(Y^eta))^(1/eta)
}

my_weibul_fun <- function(Y) {

  Y <- Y
  eta_fun <- function(eta) {
    (1/eta[1]) - (sum(Y^eta[1] * log(Y)) / sum(Y^eta[1])) + (1/length(Y))*sum(log(Y))
  }
  eta <- optim(par = 2,
               fn = eta_fun, 
               method = "CG")$par
  
  alpha <- alpha_fun(Y, eta=eta)
  
  my_weibul_params <- list("alpha" = alpha, "eta" = eta)
  my_weibul_params
  
  # for comparison
  weibul_fit <- EnvStats::eweibull(Y, method = "mle")
  weibul_fit$parameters
  
  return(list("alpha" = alpha, "eta" = eta, 
              "eweibul_alpha" = weibul_fit$parameters[[2]],
              "eweibul_eta" = weibul_fit$parameters[[1]]))
}
g1_stuff$weibul <- my_weibul_fun(Y=gps[[1]])
g2_stuff$weibul <- my_weibul_fun(Y=gps[[2]])
g3_stuff$weibul <- my_weibul_fun(Y=gps[[3]])
g4_stuff$weibul <- my_weibul_fun(Y=gps[[4]])



```



































